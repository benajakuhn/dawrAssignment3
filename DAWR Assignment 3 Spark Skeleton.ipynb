{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# run this command on databricks first, not necessary locally\n",
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "38924a7b-7cef-4ea1-a4d6-2c43be890d10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Part 1 - Parsing Hikes\n",
    "\n",
    "In the first part of the assignment, you need to extract the relevant attributes from the web pages scraped from hikr.org. Extend the `parse` function so that it extracts all the attributes you need to create the ranking. You may define your own helper functions and extend the `parse` function as necessary. Just keep in mind that the arguments/result types should not be changed to enable you to use the function in the second part of the assignment.\n",
    "\n",
    "## Chosen Features\n",
    "The follwing features are extracted from the hikr.org tour pages:\n",
    "\n",
    "### 1. Region (`region`)\n",
    "A string representing the tour's geographical region, as a breadcrumb path (e.g., \"World » Italy » Lombardy\") <br>\n",
    "**Cleaning:** Individual parts are whitespace-trimmed and joined by \" » \". It will be `None` if not found or if all parts are empty.\n",
    "\n",
    "### 2. Tour Date (`tour_date`)\n",
    "The date of the tour, formatted as a \"dd.mm.yyyy\" string (e.g., \"05.06.2010\"). <br>\n",
    "**Cleaning:** The day and month are zero-padded. It will be `None` if the original date string is not found, is in an unexpected format, or contains an unrecognized month name.\n",
    "\n",
    "### 3. Descent in Meters (`descent_meters`)\n",
    "The total descent of the tour in meters. <br>\n",
    "**Cleaning:** The \"m\" unit removed and all letters converted to lowercase (e.g., \"600\"). Leading/trailing whitespace removed. It will be `None` if not found.\n",
    "\n",
    "### 4. Ascent in Meters (`ascent_meters`)\n",
    "The total ascent of the tour in meters. <br>\n",
    "**Cleaning:** The \"m\" unit removed and all letters converted to lowercase (e.g., \"600\"). Leading/trailing whitespace removed. It will be `None` if not found."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "716140a3-fd70-489b-af13-73e2edd565bb",
     "showTitle": false,
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2025-05-21T13:00:03.507048Z",
     "start_time": "2025-05-21T13:00:02.778296Z"
    }
   },
   "source": [
    "import scrapy\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "# Parses a hikr.org tour and extracts all the attributes we are interested in.\n",
    "# Parameters:\n",
    "#   tour: HTML Content of the hikr.org tour.\n",
    "# Result:\n",
    "#   A dictionary containing the extracted attributes for this tour.\n",
    "def parse(tour):\n",
    "    # id is the filename, text is the file content\n",
    "    [id, text] = tour\n",
    "\n",
    "    # Parse it using scrapy\n",
    "    document = Selector(text=text)\n",
    "\n",
    "    name_raw = document.css('h1.title::text').get()\n",
    "    # Clean: remove leading/trailing whitespace. If not found, it remains None.\n",
    "    name = name_raw.strip() if name_raw else None\n",
    "\n",
    "    # 1. Region\n",
    "    region_xpath = '//tr[td[@class=\"fiche_rando_b\" and contains(normalize-space(.), \"Region:\")]]/td[@class=\"fiche_rando\"]//a/text()'\n",
    "    region_parts_raw = document.xpath(region_xpath).getall()\n",
    "    # Clean: Strip whitespace from each part, filter out any empty strings, then join with \" » \".\n",
    "    # If no parts are found, region remains None.\n",
    "    region = None\n",
    "    if region_parts_raw:\n",
    "        cleaned_parts = [part.strip() for part in region_parts_raw if part.strip()]\n",
    "        if cleaned_parts:\n",
    "            region = ' » '.join(cleaned_parts)\n",
    "\n",
    "    # 2. Tour Date\n",
    "    tour_date_xpath = '//tr[td[@class=\"fiche_rando_b\" and contains(normalize-space(.), \"Tour Datum:\")]]/td[@class=\"fiche_rando\"]/text()'\n",
    "    tour_date_raw_str = document.xpath(tour_date_xpath).get()\n",
    "\n",
    "    tour_date = None\n",
    "    if tour_date_raw_str:\n",
    "        # Clean: remove leading/trailing whitespace.\n",
    "        cleaned_date_str = tour_date_raw_str.strip()\n",
    "        # German month names to month number mapping\n",
    "        german_months = {\n",
    "            'Januar': 1, 'Februar': 2, 'März': 3, 'April': 4, 'Mai': 5, 'Juni': 6,\n",
    "            'Juli': 7, 'August': 8, 'September': 9, 'Oktober': 10, 'November': 11, 'Dezember': 12\n",
    "        }\n",
    "        parts = cleaned_date_str.split()\n",
    "        if len(parts) == 3:\n",
    "            day = int(parts[0])\n",
    "            month_name = parts[1]\n",
    "            year = int(parts[2])\n",
    "\n",
    "            month_number = german_months.get(month_name)\n",
    "\n",
    "            if month_number:\n",
    "                tour_date = f\"{day:02d}.{month_number:02d}.{year}\"\n",
    "\n",
    "    descent_xpath = '//tr[td[@class=\"fiche_rando_b\" and contains(normalize-space(.), \"Abstieg:\")]]/td[@class=\"fiche_rando\"]/text()'\n",
    "    descent_raw_str = document.xpath(descent_xpath).get()\n",
    "    # Clean: convert to lowercase, remove \"m\", and strip whitespace.\n",
    "    descent_meters = None\n",
    "    if descent_raw_str:\n",
    "        descent_meters = descent_raw_str.lower().replace('m', '').strip()\n",
    "\n",
    "    # 4. Ascent\n",
    "    ascent_xpath = '//tr[td[@class=\"fiche_rando_b\" and contains(normalize-space(.), \"Aufstieg:\")]]/td[@class=\"fiche_rando\"]/text()'\n",
    "    ascent_raw = document.xpath(ascent_xpath).get()\n",
    "    # Clean: convert to lowercase, remove \"m\", and strip whitespace.\n",
    "    ascent_meters = None\n",
    "    if ascent_raw:\n",
    "        ascent_meters = ascent_raw.lower().replace('m', '').strip()\n",
    "\n",
    "    # Assemble the result dictionary\n",
    "    result = {\n",
    "        'name': name,\n",
    "        'region': region,\n",
    "        'tour_date': tour_date,\n",
    "        'descent_meters': descent_meters,\n",
    "        'ascent_meters': ascent_meters\n",
    "    }\n",
    "\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9768e05e-f8e8-4dce-9979-e734f897cb6e",
     "showTitle": false,
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2025-05-21T13:00:04.243207Z",
     "start_time": "2025-05-21T13:00:03.525997Z"
    }
   },
   "source": [
    "# Extract the 200posts.zip file in the same folder where this jupyter notebook is located.\n",
    "# Then you can run the parse function on an example tour:\n",
    "with open('200posts/post24013.html', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    r = parse([f.name, content])\n",
    "    print(r)\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '200posts/post24013.html'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Extract the 200posts.zip file in the same folder where this jupyter notebook is located.\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# Then you can run the parse function on an example tour:\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m200posts/post24013.html\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m      4\u001B[39m     content = f.read()\n\u001B[32m      5\u001B[39m     r = parse([f.name, content])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:326\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    321\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    322\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    323\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    324\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: '200posts/post24013.html'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72a2c7d6-16ce-4c96-8841-41ebfd923fd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Part 2 - Parallelization & Aggregation (Spark)\n",
    "\n",
    "It is highly recommended to wait with this part until after the Spark lecture!\n",
    "\n",
    "This part only works on databricks!\n",
    "\n",
    "Warning: In the community edition, databricks terminates your cluster after 2 hours of inactivity. If you re-create the cluster, you will lose your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a library such as scrapy, it might not always work with the command above. Should you run into problems, you can alternatively do the following:\n",
    "\n",
    "- Go to the \"Clusters\" panel on the left\n",
    "- Select your cluster\n",
    "- Go to the \"Libraries\" tab\n",
    "- Click \"Install New\"\n",
    "- Choose \"PyPI\" as library source\n",
    "- Type the name of the library, \"scrapy\", into the package field\n",
    "- Click \"Install\"\n",
    "- Wait until the installation has finished\n",
    "\n",
    "You can now use the newly installed library in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76eabc05-2c4a-47da-92b5-169e279fa5b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AWS Access configuration\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"AKIAYFVAOB5OOWVMUSCZ\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"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\")\n",
    "\n",
    "# Contains the whole hikr dataset.\n",
    "# The full dataset contains 42330 tours and has a size of around 3 GB. Use this dataset for your final results if possible.\n",
    "# Execution is likely to take around 20 to 30 minutes.\n",
    "# tours = sc.wholeTextFiles(\"s3a://dawr-hikr3/hikr/*.html\")\n",
    "\n",
    "# There are 8176 posts starting with \"post10*\", which is a nicer size for smaller experiments. (~ 5 minutes to process)\n",
    "# tours = sc.wholeTextFiles(\"s3a://dawr-hikr3/hikr/post10*.html\")\n",
    "\n",
    "# If you want to further shrink the dataset size for testing, you can add another zero (or more) to the pattern (post100*.html).\n",
    "tours = sc.wholeTextFiles(\"s3a://dawr-hikr3/hikr/post100*.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "969dcc15-b5bb-4763-b10d-8188f446fabc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply our parse function and persist the parse results so that we can repeat all further steps easier\n",
    "import pyspark\n",
    "parsedTours = tours.map(parse).persist(pyspark.StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "98c18122-0ca9-4421-a7a0-e5cbe2bf9746",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# actually force the parsedTours RDD. Above it was only defined, but not evaluated. This will take a while.\n",
    "parsedTours.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0d47eb26-ae76-4404-b2ad-10e48dfd03e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "# Add your code here. Note that executing this cell and any below can reuse the results from \"parsedTours\".\n",
    "\n",
    "# Example - let's just collect everything\n",
    "parsedTours.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f508f8d5-64bb-4744-8941-067ad0db8c16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 2 Final ranking\n",
    "List your final top 10 mountain peaks that occur the most often within your filtered tours. State how you handle cases where two peaks occur the same number of times."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 3 - Analysis of data quality\n",
    "Add further code for analysis of data quality here. Don't forget to include at least one aggregation, such as average tour length per season."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DAWR Assignment 3 Spark Skeleton",
   "notebookOrigID": 3995684355508196,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
